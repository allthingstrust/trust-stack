# Trust Stack Rating Tool ‚Äî Architecture Overview

> **Purpose**: This document provides a comprehensive overview of the codebase architecture for new developers joining the project.

---

## üéØ System Overview

The **Trust Stack Rating Tool** is a content quality assessment system that analyzes brand-linked content across digital channels using a **5D Trust Framework**. It collects content from multiple sources, scores it across five trust dimensions, and generates actionable reports.

```mermaid
flowchart TB
    subgraph Input["üîç Data Collection"]
        Search[Web Search APIs]
        Social[Social Media APIs]
        Direct[Direct URL Fetching]
    end
    
    subgraph Core["‚öôÔ∏è Processing Pipeline"]
        Ingest[Ingestion Layer]
        Normalize[Normalization]
        Enrich[Enrichment & Attribute Detection]
        Score[5D Scoring Engine]
        Classify[Classification]
    end
    
    subgraph Output["üìä Outputs"]
        DB[(SQLite Database)]
        Reports[PDF/Markdown Reports]
        WebApp[Streamlit Dashboard]
    end
    
    Input --> Ingest --> Normalize --> Enrich --> Score --> Classify --> Output
```

---

## üìÅ Directory Structure

```
authenticity-ratio/
‚îú‚îÄ‚îÄ config/                 # Configuration and settings
‚îÇ   ‚îú‚îÄ‚îÄ settings.py         # Global settings, API configs, scoring weights
‚îÇ   ‚îú‚îÄ‚îÄ rubric.json         # 5D scoring rubric with signals and thresholds
‚îÇ   ‚îî‚îÄ‚îÄ brand_guidelines/   # Brand-specific guideline files
‚îÇ
‚îú‚îÄ‚îÄ core/                   # Core orchestration
‚îÇ   ‚îî‚îÄ‚îÄ run_manager.py      # Pipeline run management and orchestration
‚îÇ
‚îú‚îÄ‚îÄ data/                   # Data layer
‚îÇ   ‚îî‚îÄ‚îÄ models.py           # SQLAlchemy ORM models + legacy dataclasses
‚îÇ
‚îú‚îÄ‚îÄ ingestion/              # Data collection layer
‚îÇ   ‚îú‚îÄ‚îÄ brave_search.py     # Brave Search API integration
‚îÇ   ‚îú‚îÄ‚îÄ serper_search.py    # Serper (Google) Search API
‚îÇ   ‚îú‚îÄ‚îÄ reddit_crawler.py   # Reddit API integration
‚îÇ   ‚îú‚îÄ‚îÄ youtube_scraper.py  # YouTube Data API v3
‚îÇ   ‚îú‚îÄ‚îÄ page_fetcher.py     # HTTP/Playwright page fetching
‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py       # Content standardization
‚îÇ   ‚îú‚îÄ‚îÄ metadata_extractor.py   # Metadata extraction
‚îÇ   ‚îú‚îÄ‚îÄ domain_classifier.py    # Brand-owned vs third-party classification
‚îÇ   ‚îú‚îÄ‚îÄ whois_lookup.py     # WHOIS data for domain provenance
‚îÇ   ‚îî‚îÄ‚îÄ playwright_manager.py   # Browser automation for JS-heavy pages
‚îÇ
‚îú‚îÄ‚îÄ scoring/                # Scoring engine
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py         # Main ScoringPipeline orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ scorer.py           # ContentScorer (5D dimension scoring)
‚îÇ   ‚îú‚îÄ‚îÄ attribute_detector.py   # Trust attribute detection (SSL, schema, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ aggregator.py       # Score aggregation across content items
‚îÇ   ‚îú‚îÄ‚îÄ classifier.py       # Content classification (Excellent/Good/Fair/Poor)
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py       # LLM API integration for scoring
‚îÇ   ‚îú‚îÄ‚îÄ key_signal_evaluator.py # Key signal evaluation
‚îÇ   ‚îî‚îÄ‚îÄ rubric.py           # Rubric loading and interpretation
‚îÇ
‚îú‚îÄ‚îÄ reporting/              # Report generation
‚îÇ   ‚îú‚îÄ‚îÄ pdf_generator.py    # PDF report generation
‚îÇ   ‚îú‚îÄ‚îÄ markdown_generator.py   # Markdown report generation
‚îÇ   ‚îú‚îÄ‚îÄ executive_summary.py    # Executive summary generation
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py        # Dashboard utilities
‚îÇ
‚îú‚îÄ‚îÄ webapp/                 # Streamlit web application
‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Main Streamlit application
‚îÇ   ‚îú‚îÄ‚îÄ pages/              # Multi-page navigation
‚îÇ   ‚îú‚îÄ‚îÄ services/           # Business logic services
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # UI utilities
‚îÇ
‚îú‚îÄ‚îÄ utils/                  # Shared utilities
‚îÇ   ‚îú‚îÄ‚îÄ helpers.py          # General helper functions
‚îÇ   ‚îú‚îÄ‚îÄ logging_config.py   # Logging configuration
‚îÇ   ‚îî‚îÄ‚îÄ document_processor.py   # Document processing utilities
‚îÇ
‚îú‚îÄ‚îÄ scripts/                # CLI tools and utilities
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py     # CLI pipeline runner
‚îÇ
‚îú‚îÄ‚îÄ tests/                  # Test suite
‚îî‚îÄ‚îÄ docs/                   # Documentation
```

---

## üîÑ Pipeline Flow

### 1. Ingestion Layer (`ingestion/`)

Collects content from configured data sources:

| Module | Purpose | Key Features |
|--------|---------|--------------|
| [brave_search.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/ingestion/brave_search.py) | Brave Search API | HTML fallback, rate limiting |
| [serper_search.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/ingestion/serper_search.py) | Google Search via Serper | Structured results |
| [reddit_crawler.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/ingestion/reddit_crawler.py) | Reddit API | OAuth, subreddit filtering |
| [youtube_scraper.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/ingestion/youtube_scraper.py) | YouTube Data API v3 | Video + comment analysis |
| [page_fetcher.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/ingestion/page_fetcher.py) | HTTP/Playwright fetching | JS rendering, robots.txt compliance |

**Data Flow**:
```
Search Query ‚Üí Search APIs ‚Üí URLs ‚Üí Page Fetcher ‚Üí Raw HTML ‚Üí Content Extraction
```

### 2. Normalization (`ingestion/normalizer.py`)

Standardizes content into `NormalizedContent` dataclass with fields:
- `content_id`, `src`, `platform_id`, `author`, `title`, `body`
- `url`, `channel`, `source_type`, `source_tier`
- `meta` dictionary for additional attributes
- `structured_body` for HTML structure preservation

### 3. Enrichment (`scoring/attribute_detector.py`)

Detects 36+ trust attributes across dimensions:
- **Provenance**: SSL, WHOIS age, author presence, schema.org markup
- **Verification**: Fact-check mentions, official badges
- **Transparency**: Disclosure statements, terms/privacy links
- **Coherence**: Brand voice consistency, content freshness
- **Resonance**: Engagement patterns, cultural fit

### 4. Scoring Engine (`scoring/`)

```mermaid
flowchart LR
    Content[NormalizedContent]
    
    subgraph Scorer["ContentScorer"]
        LLM[LLM-based Scoring]
        Heuristic[Heuristic Checks]
        Attrs[Attribute Detection]
    end
    
    Content --> Scorer
    Scorer --> Dims[5D Dimension Scores]
    Dims --> Weighted[Weighted Average]
    Weighted --> Classification
```

**Key Classes**:

| Class | File | Responsibility |
|-------|------|----------------|
| `ContentScorer` | [scorer.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/scoring/scorer.py) | Scores content on 5 dimensions |
| `ScoringPipeline` | [pipeline.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/scoring/pipeline.py) | Orchestrates full scoring workflow |
| `TrustStackAttributeDetector` | [attribute_detector.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/scoring/attribute_detector.py) | Detects 36+ trust attributes |

### 5. Classification (`scoring/classifier.py`)

Maps scores to rating bands:
- **Excellent** (80-100): High-quality, verified content üü¢
- **Good** (60-79): Solid content with minor improvements üü°
- **Fair** (40-59): Moderate quality requiring attention üü†
- **Poor** (0-39): Low-quality content needing review üî¥

---

## üìä Data Models

Located in [data/models.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/data/models.py):

### SQLAlchemy ORM Models (New)

| Model | Table | Purpose |
|-------|-------|---------|
| `Brand` | `brands` | Brand being analyzed |
| `Scenario` | `scenarios` | Analysis scope/playbook |
| `Run` | `runs` | Pipeline execution record |
| `ContentAsset` | `content_assets` | Individual scored asset |
| `DimensionScores` | `dimension_scores` | Per-asset dimension scores |
| `TrustStackSummary` | `truststack_summary` | Aggregated run metrics |

### Legacy Dataclasses (Backward Compatibility)

| Dataclass | Purpose |
|-----------|---------|
| `NormalizedContent` | Standardized content representation |
| `ContentScores` | Dimension scores (0.0-1.0 scale) |
| `DetectedAttribute` | Trust attribute detection result |
| `TrustStackRating` | Per-item trust rating |
| `AuthenticityRatio` | Legacy AR metric (deprecated) |
| `PipelineRun` | Pipeline execution tracking |

---

## üåê Web Application

The Streamlit app ([webapp/app.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/webapp/app.py)) provides:

1. **Home Page**: Overview and data source status
2. **Analyze Page**: Brand configuration, URL collection, analysis execution
3. **Results Page**: Visualizations, dimension breakdown, content table
4. **History Page**: Past analysis runs and comparison

**Key Functions**:
- `infer_brand_domains()` ‚Äî Auto-detect brand domains
- `suggest_brand_urls_from_llm()` ‚Äî LLM-powered URL discovery
- `show_analyze_page()` ‚Äî Main analysis workflow
- `show_results_page()` ‚Äî Results visualization

---

## ‚öôÔ∏è Configuration

### Settings ([config/settings.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/config/settings.py))

```python
SETTINGS = {
    'scoring_weights': ScoringWeights(),  # 5D dimension weights (default: 0.2 each)
    'rating_bands': {
        'excellent': 80,
        'good': 60,
        'fair': 40,
        'poor': 0,
    },
    ...
}
```

### Rubric ([config/rubric.json](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/config/rubric.json))

Defines signals and thresholds for each dimension. Used by `ContentScorer` to determine scoring weights and multipliers by content type.

### Environment Variables

```bash
# LLM APIs
OPENAI_API_KEY=...
LLM_MODEL=gpt-4o-mini

# Search Providers
BRAVE_API_KEY=...
SERPER_API_KEY=...

# Social APIs
REDDIT_CLIENT_ID=...
YOUTUBE_API_KEY=...

# Feature Flags
AR_USE_PLAYWRIGHT=1
```

---

## üîå External Dependencies

| Category | Service | Purpose |
|----------|---------|---------|
| **LLM** | OpenAI GPT | Content scoring, recommendations |
| **Search** | Brave, Serper | Web content discovery |
| **Social** | Reddit API, YouTube API | Social platform content |
| **Browser** | Playwright | JavaScript rendering |
| **Storage** | SQLite (local), S3/Athena (optional) | Data persistence |

---

## üß™ Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific module tests
pytest tests/test_scoring.py -v
```

Test files are in `tests/` and cover:
- Ingestion modules
- Scoring logic
- Classification
- Integration flows

---

## üì§ Report Generation

Reports are generated in `reporting/`:

| Format | Generator | Output |
|--------|-----------|--------|
| PDF | [pdf_generator.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/reporting/pdf_generator.py) | Visual report with charts |
| Markdown | [markdown_generator.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/reporting/markdown_generator.py) | Text-based report |
| JSON | Built-in | Machine-readable data |

Output location: `output/webapp_runs/{brand_id}_{run_id}/`

---

## üöÄ Quick Start for Developers

1. **Setup**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   cp .env.example .env  # Add API keys
   ```

2. **Run Web App**:
   ```bash
   streamlit run webapp/app.py
   ```

3. **Run CLI Pipeline**:
   ```bash
   python scripts/run_pipeline.py <brand> web "<keywords>"
   ```

4. **Key Files to Explore**:
   - Start with [webapp/app.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/webapp/app.py) for the user flow
   - [scoring/pipeline.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/scoring/pipeline.py) for the scoring workflow
   - [data/models.py](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/data/models.py) for data structures

---

## üìö Additional Documentation

- [Webapp README](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/webapp/README.md) ‚Äî Web application guide
- [AR Methodology](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/docs/AR_METHODOLOGY.md) ‚Äî Scoring methodology details
- [Search Architecture](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/docs/SEARCH_ARCHITECTURE.md) ‚Äî Search provider details
- [Model Selection Guide](file:///Users/andrewdeutsch/Documents/AR/authenticity-ratio/MODEL_SELECTION_GUIDE.md) ‚Äî LLM model comparison
